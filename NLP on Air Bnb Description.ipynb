{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f83e0f7",
   "metadata": {},
   "source": [
    "# Text Mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e86f69",
   "metadata": {},
   "source": [
    "For this project, we will be using the Latent Dirichlet Allocation (LDA) model to uncover hidden structures in the description variable for each dataset. The description variable wil be visualized in both a wordcloud and intertopic distance map. These graphs will help visualize the most frequent terms for the description of each listing.\n",
    "\n",
    "The Wordclouds, HTML files of the Intertopic Distance Maps and raw data files can all be found in the folders on the main branch.\n",
    "\n",
    "To run for each city, the df must changed for the corpus variable. For example, corpus = df_tor[descript] would be used for Toronto whereas corpus = df_van[descript] would be used for Vancouver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ff85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e79dcb",
   "metadata": {},
   "source": [
    "First, given there are lots of rows in the CSV file. I wanted to separate out the description variable into its own dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba77327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>*** Unit upgraded with new bamboo flooring, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>Run Runyon Canyon, Our Gym &amp; Sauna Open Beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>An oasis of tranquility awaits you.The spaceTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>Centrally located.... Furnished with King Size...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>Our home is located near Venice Beach without ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description\n",
       "id                                                     \n",
       "109   *** Unit upgraded with new bamboo flooring, br...\n",
       "2708  Run Runyon Canyon, Our Gym & Sauna Open Beauti...\n",
       "2732  An oasis of tranquility awaits you.The spaceTh...\n",
       "2864  Centrally located.... Furnished with King Size...\n",
       "5728  Our home is located near Venice Beach without ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = ('LA_listings.csv') #Data file to load in\n",
    "df = pd.read_csv(path, header=0, index_col=0)\n",
    "descript = ['description']\n",
    "corpus = df[descript] #Dataframe with just the description\n",
    "corpus.head() #Check the df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0253e2",
   "metadata": {},
   "source": [
    "Now that we have our own description dataframe, we must clean up the data. We'll be using Tokenization & Lemmatization. Each term will be chopped up into \"tokens\" and certain things that we don't want will be removed such as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(corpus): #Need to standardize the text (remove stopwords, punctuation, capitalization, lemmatization etc)\n",
    "    clean_corpus = []\n",
    "    en_words = set(nltk.corpus.words.words())\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|!]+')\n",
    "    for row in corpus:\n",
    "        word_tokens = tokenizer.tokenize(row)\n",
    "        word_tokens_lower = [t.lower() for t in word_tokens]\n",
    "        word_tokens_lower_en = [t for t in word_tokens_lower if t in en_words or not t.isalpha()]\n",
    "        word_tokens_no_stops = [t for t in word_tokens_lower_en if not t in en_stopwords]\n",
    "        word_tokens_no_stops_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in word_tokens_no_stops]\n",
    "        clean_corpus.append(word_tokens_no_stops_lemmatized)\n",
    "    return clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82e6e4",
   "metadata": {},
   "source": [
    "Now we want to create the document-term matrix of our description corpus. Each row in the document-term matrix, is a vector, with one column for every term in the matrix. This tracks the term frequency for each term in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6edfafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_model_pipeline(clean_corpus): #Create a dictionary for the cleaned words\n",
    "    dictionary = Dictionary(clean_corpus)\n",
    "    doc_term_matrix = [dictionary.doc2bow(listing) for listing in clean_corpus]    \n",
    "    return dictionary, doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6147150",
   "metadata": {},
   "source": [
    "LDA is a topic model. Words from the document, in our case the description of Air Bnbs can be divided into topics. In these examples, I used 3 topics total as that was where I got the least amount of overlap between each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91154c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_topic_modelling(doc_term_matrix, dictionary, num_topics=3, passes=2): #LDA Model\n",
    "    LDA = LdaModel\n",
    "    ldamodel = LDA(doc_term_matrix, num_topics=num_topics, id2word = dictionary, passes=passes)\n",
    "    return ldamodel\n",
    "\n",
    "def add_topics_to_df(ldamodel, doc_term_matrix, df, new_col, num_topics): #Create new Df with Topics assigned\n",
    "    docTopicProbMat = ldamodel[doc_term_matrix]\n",
    "    docTopicProbDf = pd.DataFrame(index=df.index, columns=range(0, num_topics))\n",
    "    for i, doc in enumerate(docTopicProbMat):\n",
    "        for topic in doc:\n",
    "            docTopicProbDf.iloc[i, topic[0]] = topic[1]\n",
    "    docTopicProbDf[new_col] = docTopicProbDf.idxmax(axis=1)\n",
    "    df_topics = docTopicProbDf[new_col]\n",
    "    df_new = pd.concat([df, df_topics], axis=1)\n",
    "    return df_new\n",
    "\n",
    "corpus_description = corpus['description'].astype(str)\n",
    "new_corpus = preprocess_text(corpus_description)\n",
    "dictionary_description, doc_term_matrix_description = nlp_model_pipeline(new_corpus)\n",
    "ldamodel_description = LDA_topic_modelling(doc_term_matrix_description, dictionary_description, num_topics=3, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7ffe8",
   "metadata": {},
   "source": [
    "Intertopic Distance Plots can be found on the main branch. This allows us to explore words that were classified into each of the 3 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c89b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conno\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "ch = gensimvis.prepare(ldamodel_description, doc_term_matrix_description, dictionary_description)\n",
    "pyLDAvis.save_html(ch, 'laLDA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175c97e",
   "metadata": {},
   "source": [
    "3 Topics\n",
    "\n",
    "Budget: Contains words such as room, bed, apartment. Basic amenities most homes have.\n",
    "Location: Contains words such as walking distance, subway, bus, downtown. Focus around travel and being close to the tourist-heavy areas.\n",
    "Luxury: Contains words such as modern, unique, luxury. High-end features that most homes don’t have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fee12d",
   "metadata": {},
   "source": [
    "In addition to the intertopic distance maps, I created a WordCloud to show the most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a15e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "canvas_width=1280\n",
    "canvas_height=720\n",
    "long_string = ','.join(list(corpus_description))\n",
    "wordcloud = WordCloud(width=canvas_width,height=canvas_height,background_color=\"white\", max_words=50000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_file('LA_wordcloud.png') #Change name based on dataset you're using\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "img = mpimg.imread('la_wordcloud.png') #LA Wordcloud\n",
    "imgplot = plt.imshow(img)\n",
    "x_axis = imgplot.axes.get_xaxis()\n",
    "x_axis.set_visible(False)\n",
    "\n",
    "y_axis = imgplot.axes.get_yaxis()\n",
    "y_axis.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c926f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c8a8f",
   "metadata": {},
   "source": [
    "Common words/phrases in all eight cities appear to be around the city name as well as the dwelling type. Home, House & Apartment all appeared very frequently, and in certain datasets such as Toronto, they appear the most. Given the description is a place where the lister would be describing what type of dwelling the home is in the description, this makes sense logically. Words like kitchen, shower, living room also appear often, allowing for the assumption that most Air Bnb's have basic amenities that a guest would need for their short-term stay. Another theme that can be derived from the word clouds is words related to a location such as the heart of the city, downtown, subway, walking distance & access. Using this data one can conclude that a large portion of Air Bnb’s are in a location in major tourist areas of the city or close to transportation allowing people to get to these tourist areas. Given the eight cities for this report are all very large cities, this also makes sense logically. They all have tourist areas and the people traveling to these cities using Air Bnb's are likely not from the area and are traveling to the city for leisure. Both Chicago and Sydney have the license number prominent in the description whereas other cities do not. This is likely due to Airbnb regulations imposed on those two cities that hosts must include this information in their listing\n",
    "\n",
    "Canadian cities tend to have a higher percentage of homes in the luxury bin compared to other worldwide cities. For the non-Canadian cities, the Budget and Location groups cover almost the dataset in its entirety with the exception of LA. When exploring the latitude/longitude coordinates in the EDA stage, LA didn’t have a specific cluster where properties tended to cluster so it makes sense why the Location percentage isn’t as high. From the intertopic distance maps, we can infer that the majority of Airbnb renters are looking for a budget property in an area near the heart of the city center.\n",
    "\n",
    "Topic Breakdown from LDA Analysis\n",
    "\n",
    "Toronto\n",
    "Budget:51%\n",
    "Location:25%\n",
    "Luxury: 24%\n",
    "\n",
    "Montreal\n",
    "Budget:47%\n",
    "Location:30%\n",
    "Luxury:23%\n",
    "\n",
    "Vancouver\n",
    "Budget:47%\n",
    "Location:33%\n",
    "Luxury:20%\n",
    "\n",
    "Barcelona\n",
    "Budget:45%\n",
    "Location:41%\n",
    "Luxury:14%\n",
    "\n",
    "LA\n",
    "Budget:68%\n",
    "Location:18%\n",
    "Luxury:14%\n",
    "\n",
    "Chicago\n",
    "Budget:46%\n",
    "Location:40%\n",
    "Luxury:14%\n",
    "\n",
    "Stockholm\n",
    "Budget:50%\n",
    "Location:41%\n",
    "Luxury:9%\n",
    "\n",
    "Sydney\n",
    "Budget:48%\n",
    "Location:45%\n",
    "Luxury:7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2782037",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cdeaa",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
